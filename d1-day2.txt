Threads 
Spark 

1. New machine have been added (Spark Cluster)
   Vmware workstation 
   Start-> Vmware Workstation -> Launch 
   you should have Centos 7 64 Bit 

   Centos --> Start
   spark : spark (this is sudo user)=> 

2.  the root  :  root123 

3. on you desktop on the shortcut \tools 
	if you have the tools folder 
	Desktop : scala shared folder \tools 
	a. Winutils-master
	b.  spark 2.4.1 bin hadoop 2.7 
------------------------------------------------------
What is thread in scala and 
multi threading :- 
parallel 

spark/spark 




Threads parallel processing 
Scala is writting in java --> 
it follows the java conncurency 

A unit of program that runs parellel is known thread  
Large data --> We need the same to processed parellel 
distributed data processing 
a local multithreaded data processing 

100000000 Records 
each record takes 100 ms to process (cpu)
make prallel processing --> 

i will have break the data  10 sets 
process 10 sets of data parallel (100ms)
120 ms per record  10 // the whoprocessing in 1/10 of  the total --> 

Threads --> scala application running you will need to 
there a limit of memory and cpu on a single box 
one single program --> Heap (Size)--> //cpu on single box runs exhaust --> 100%

a) to distribute ---> distributed system (Spark)rdd
break data and send it to multiple machines and multiple (Node )will process the the chunks and send back a consolidated response 

Storage : Which storage ? should i use for such big data (HDFS)

-Thread 
	peice of code and has run method defined inside 

-Runnbale

class Thread 
	def run(){
		/// whatever you write inside body of the 
		runs as thread 
	}

Q1. how can i create a thread 
Q2.  How can i start  thread 
Q3.  How can i run multiple threads 
Q4.  WHat if my run logic is somewhere lese how do i make run inside the thread 
/// one way to create a thread 
val thread = new Thread {
	
	override def run {
	   whatever code i write heree ? when will it run 
	}
}

thread.start // start is trigger that will submit the thread to to become runnable ---> and if the cpu timeslice is available the thread will run 


read a file and file has 1mn records 
i coudl split in /10 and create smaller list 
i could provide each list to a thread and expect 
it complet work and return/store result 
this scenario qualifies for faster processing provided 
i have a enough cpu power 


How to create a thread 

val thread1 = new Thread {
	
	override def run {
	// kickoff 
			// your code goes here 
	}
}

// how to start a thread // 
thread1.start();  // start the thread 

val thread2  = new Thread {
	override def run {
		// another business logic 

	}
}

thread2.start(); 






mSc --> 10 mb --> cdr 
pipeline configurable number of thread 

read -->filter --> chargeble--> warehouse --> billing
read -->filter --> chargeble--> warehouse --> billing
read -->filter --> chargeble--> warehouse --> billing
read -->filter --> chargeble--> warehouse --> billing
read -->filter --> chargeble--> warehouse --> billing
read -->filter --> chargeble--> warehouse --> billing
read -->filter --> chargeble--> warehouse --> billing
read -->filter --> chargeble--> warehouse --> billing
read -->filter --> chargeble--> warehouse --> billing





Intel Cpu -- 
1 CPU = 2 cores dual core 	1 core = 2 dedicated 
1 CPU = 4 Cores quad core 
1 cpu = 5 Cores Hexa Core 
1 Cpu = 8 Cores Octa Core 

2 os thread == 32K Java Thread (Time Slicing )
this means you will need memory also .... 

// MyThread.scala 
// code that contains the thread business logics
class MyThread extends Runnable {
	
	def run {
		// business logic goes here 
	}
}

// MyThreadDemo.scala 

// thread start stop logic 
object MyThreadDemo extends App {
	
	val t1 = new Thread(new MyThread);
	t1.start(); 
}























To run Spark on  Windows it rquires hadoop binaries 
due to limitation of GNU-Utils and Hadoop being developed for Linux :- 
The libraries hadoop are not available on windows 
Replacer library )(WINUtils----> )




Step 1 : on your desktop you have a shared folder 
Step 2 : Extract the winutils and and copy the 
			hadoop2.7.1 to your c drive 
			note: This folder becomes your 
				HADOOP_HOME 
Step 3 : SET up spark directory by extracting the 
			SPARK_xxxx
if you have the spark sheel 

first read a file : 
   val airports = spark.read.textFile("path");
   val headers  = airports.first ;  // store the 
   									// first line 
   									// in the val header
   val noheadersdata = airports.filter(line => line ! header);

   println(noheadersdata.count);
   println(noheadersdata.first);



To make you scala code with with spark you will need to provide the dependencies to scala: 
Scala dependencies we discussed are generally the classpath we set : 
Jar files / maven settings / Sbt settings 

jar files --> 


Working with Cluster 


The username and password to login to your linux box 
is 
username : spark 
password : spark 
--------------------
rootuser : root 
rootpwd  : root123 
--------------------
All the installations are in /opt 


cd /opt/spark-2.4.3-bin-hadoop2.7/sbin
./start-master.sh 
sudo service firewalld stop

ifconfig //get (ens33) ip // 

on your host machine http://ip:8080
./start-slave.sh spark://sparkmaster:7077
worker should start appearing ,. 












































Day 5 : 

a) Learnt what is scala 
b) what is repl
c) what is object (Singleton)
d) what is class 
e) constructors 
f) traits 
g) we can functions 
h) main methods
i) Collection 
j) classes  
i) The way how maven works 
j) how sbt works 
k) how to write scala script 
l) maven dependencies 
m) jar files 
n) spark on windows on stand alone mode 
o) how understand writing a spark application 
p) Threads and Runnable (Parellel programming )
........
-------------------------------------------------
Architecture our systems and how are we goint to achieve the things 



sparkmaster is your  master machine 
a) this machine will act as master for the spark 
b) this machine will act as primary name node server (HDFS)


worker1 
a) Act the slave machine for connecting with the spark master 
b) it also going to act as secondary name node 




own all the files and configuration by the user you 
are going fir the spark system and hadoop system 

own all the configuration files 
own all the exuctable that your run 
Command - 1 

sudo chown -R spark  /opt/*

We need to set the environment : 

bashrc / .bash_profile 
// what needs to be set in the .bash_profile



Edit you .bash_profie /.bashrc 
set your env variable 

PATH=$PATH:$HOME/.local/bin:$HOME/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin/:/opt/spark-2.4.3-bin-hadoop2.7/sbin:/opt/hadoop-2.7.3/bin:/opt/hadoop-2.7.3/sbin:/opt/hadoop-2.7.3/lib/native
export HADOOP_HOME=/opt/hadoop-2.7.3
export HADOOP_CONF_DIR=/opt/hadoop-2.7.3/etc/hadoop
export HADOOP_MAPRED_HOME=/opt/hadoop-2.7.3
export HADOOP_COMMON_HOME=/opt/hadoop-2.7.3
export HADOOP_HDFS_HOME=/opt/hadoop-2.7.3
export YARN_HOME=/opt/hadoop-2.7.3
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.el7_6.x86_64/
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native


export PATH


You now need to check and run the bash_profile 
. .bash_profile from you home dir 

env to check all environment variables are set or not 




We saw where our spark and hadoop directories were installed 
on the linux boxes 
we set the Require variables 
HADOOP_HOME
SPARK_HOME
MAPRED_HOME
YARN_HOME.... 






How to configure the hdfs namenode and the datanode 
and write some files (airports.csv to the )
hdfs 

hadoop files system (hdfs)


1. core-site.xml // ad the name 
		hdfs://sparkmaster:9000

2. yarn-site define the shuffler 

3. mapred-site 
	yarn as the scheduling 

4.  hdfs-site 
		how many replicas 
		and whats the permission 


5. in the hadoop env file 
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.el7_6.x86_64/
export JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native



6. 


1. hadoop installed /opt/hadoop-2.7.3 
2. Env Variables set 
3. configuration done (/etc/hadoop/4 files)
4. copied the configuration to the node 
5. sparkmaster in place 
6. in the shell prompt of the master 




start-dfs.sh // start dfs // 

it will start you namenode/secondarynode/datanode 




cd /opt/hadoop-2.7.3/sbin

./start-dfs.sh  // start 3 things 
name node 
data node 
secondary node 


hdfs dfs -ls / 
hdfs dfs -copyFromLocal fileName.txt / 
hdfs dfs -cat  /filename.txt 
hdfs dfs -rm  /filename.txt 

hdfs dfsadmin -report 

dfs-stop.sh 
dhs-start.sh

you have created a namenode , you created a datanode : 1

spark-defaults.conf // CONTAINT THE NAME OF THE MASTER
CONTAINTS THE LOGGING 
CONTIAINS FEW OTHE serialization settings 


log4j.properties // this for the logger 
how much verbose logger you want you determine 
from here 


Spark-env

SPARK_MASTER_HOST=sparkmaster
SPARK_MASTER_PORT=7077
JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native





To start the slave 
first your master should be running 
you should know you master url whihch is visible on the 
master page 
http://sparkmaster:8080 

spark://sparkmaster:7077 // is the masterurl 


cd /opt/spark-hadoop-2.7.3/sbin

./start-slave.sh spark://sparkmaster:7077 



7077: the spark master port : THe Master server

8080 : this is the monitoring ui on the spark master: ui 

9000 : hdfs cluster point  





Repeat the same steps on worker1
add Entries .bash_profile 
 . .bash_profile 

 copy the configue env to haddop 
 conf to the spark 

startDfs.sh on worker1 
startSlave.sh  spark://sparkmaster:7077 

1. jar that contains the job class 
2. you should name of the clas 
3. the name of master 

spark-submit --class cg.samp2.ScalaSparkDemo --master spark://sparkmaster:7077  hdfs://sparkmaster:9000/test.jar 


this would submit the job and run the job main 

















Day -6  Starting in 2 mins :-----

Spark in local format 
	Windows / Linux / Programming Environment 
				Driver
				------
				  ()
			==============
			| n1| n2 | n3 |........
			==============
				 HDFS 
			[primary] [sec]	
			  |					|
			 dnode1 ........ Dnoden
			======================	
Storage : HDFS 

spark in local : 
spark-shell : --> 

spark-shell --master spark://host:port (master)
spark-submit -- submit jobs (jarfiles)

Gist : 
Spark : 
1)  Is distributed computed -: with Threads (Parellel)
2)  SPARK --> can distribute jobs to mulitple workers 
3)  SPark can create jobs and parttion them 
	100000 --> 4 
4)  Spark has all inmemory (cache) 90% (persist)
5)  spark has the following libraries 
		a) Spark Core (Resilent Distributed Data)
		b) SPark SQL (DataFrames (Tables,views))
					//json,csv,jdbc,header and other 
		c)  MLib (Machine Learning)
		d)  Graph-X 

---------------------------------------------------------------
spark in local 

SparkConf 
// which master or what is jobname where is wish to connect 
SparkContext 
// sparkContenx the driver by which a) you can control the spark 
application a) read files , b) create rdd , creat any things that you wish do with the spark 
SparkSession : ---> Which actually extends context--> 	
	read . json , dataframe , sql , jdbc 


i want to connect to remote clluster ? 
where will i write the configuration 
a) Where is the master node 
b) what is my application job name (When submit application is can trace it from the ui)
c) what are the number, workers i should be using (*) or my defined ones 

val sparkConfig = new SparkConf().setMaster("spark://host:port").setAppName("app1");
// jdbc url , username, password ,sid 
// the spark configuration is passed to create SparkContext 
// DataBase Conecttion 
jdbc						 spark 
======================================================
driver(jdbc)				| spark-core driver
url							| master url
username,password			| enable security
connection					|  sc , sparkSession 
crud						|  sc.saveFile, readfile, queries,
exit 						|  end 
DataBase 					|  run on the number of workers 
function feature			|  spark-sql
							|  spark streaming
=========================================================
Spark Application can inherently be scala :- 

Scala Application 
	|-----src 
			|---main 
				 |---scala 
				 		|---com.cg
				 				|----App.scala 
				test
				 |--scala
				 	  |--com.cg.test
				 	  		|---UnitTest.scala 
	|---pom.xml	
		  |------dependencies 
		  				|-----spark-core
		  				|-----spark-sql
		  				|-----spark-streaming
		  				|-----spark-graphx 

		  |--- plugins 
		  			|----- shade : plugin 
		  					|---- fat jar/thin jar 
		  					a) fat jar is jar that would 
		  					be purely having all dependencies 
		  					bundled in a single jar file 
		  					// single 	

		  					b) thin jar onlu contain 
		  						my classes and not other 
		  						dependencies 

==============================================================
Where am i runing it 
a) windows (local spark system) "local"
b) what is that i am running
	spark job  ==>Submit 
	spark prog ==> locall 
c) SParkShell -> 

How to write my first Spark Application :- 
a) windows a local spark application development 
b) linux with 1 master, 1 worker which is a remote environment 
	ie. like the production 
===============================================================
spark commands 
a) bin
	spark-shell.cmd
		also starts a local cluster mode 
	spark-submit.cmd
	spark-sql.cmd
	spark-class.cmd

b) sbin // sbin is not available on windows 
===============================================================
there is a local mode the master 
development mode always :local 

there is a remote mode of the master
production mode is always : remote  
===============================================================
first Spark Program :-
a) i need a spark-core library 
	(maven dependencies)
	 <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.11</artifactId>
      <version>2.4.3</version>
    </dependency>

    because i added this now i have an access to the 
    SparkConf class 
    SparkContext Classs 
    SparkSession 


b) i need to create a spark-configuration 
	i) the name of the app 
	ii) the master to which i should connect (local)

c) using the spark configuration i can build the sparksession 	
	or the spark context 

==============================================================
spark-shell					|		    spark-app(.scala)
sc (inherent)				|		 sc not available 
sparksession(inherent)		|		no not available 
sparkother Variables        |	    they are not 
===============================================================





Set up you development environment to support programming 
of spark atleast for the RDD Purpose . 

a) add a entry to your pom.xml 
b) create a scala Object with main method 
c) add the SparkConf + then create SparkContext 
d) try creating rdd's 
----------------------------------------------------------------

We are going to write programs that will allows to do 
RDD;s

Resilient Distributed DataSet
RDD 

1000000
map // each record of the rdd when itereated can have 
		some enhancedment via map 
flatMap // convert the complete rdd in single record 
filter // if the case true --> 
reduce 

a)  sc.textFile
b)  array -->sc.parellelize(array);
c)  spark.read.json().rdd /// json /rdd 

------------------------------------------------------------------
1. when you dependecies to maven they get added to you classpath 
2. compatibility layer 
3. You added depenedency to you spark-core
<dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.11</artifactId>
      <version>2.4.3</version>
    </dependency>

4. create new scala Object : 


starting in 5 mins 



1.   spark-shell 
2.   http://localhost:4040 
3.   in the spark-shell 
		:paste 

4. paste the code form the chat windows 
5. 






Step 1 : your started a spark-shell 
Step 2 : you http://localhost:4040
Step 3 :  :paste 






We can write spark Application Scala 

SparkConf 
SparkContext 
SparkSession 


=========================================================================================
Day 7 
=========================================================================================
spark with scala :- 
1. spark is not a distributed compute system
2. spark on windws 
3. spark  on a linux cluster
4. spark on standalon code 
5. spark application with scala 
6. sc /// sparkContext 
7. sparkSession /// spark
8. spark-shell 
9. master 
10. workers 
11. hdfs usage 
12. how to read files / map/filter/take 
13. RDD
14. data is being read from a file 
15. how we could json,csv  formatter data (Schema)
16. maven /sbt/gradle 
17. jar and how the jar (use the jar and submit jobs as spark jobs ) 

Spark 
-----
Spark - Core libary
RDD --> Standard RDD function 

val sparkConf = new SparkConf().setAppName().setMaster();
val sc = SparkContext(conf);// get the confiu
sc.textFile // one way to create RDD 
sc.parellize // 
we could actuall a large data activities 
count 
filter 
map 
flatMap 
take 
RDD does not have a schematic and RDD of flat Data that i would work with 
-------------------------------------------------------------------------------------
Spark - Sql Library 
is more of schematics --> 
a) json 
b) csv 
c) parquet 
d) xml 

  <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.4.3</version>
        </dependency>
if i donot add the dependency in to my application pom.xml i will not get the 
the jar files (class files in my classpath) it bound to have a clear no class found 




RDD   	v/s   DataFrames  
--------------------------
Streaming Data .....

a) Twitter 
b) Kafka 
c) S3 
d) HDFS 
e) MQTT
f) Kinesis 
g) Log Server ---> 

// converted to a dataframe or rdd ///

	


Streaming data :- 
used for Live feeds 
a) Twitter (Developer)
b) Kafka 
c) MQTT  (IoT Devices)
d) Kinesis 
e) S3 / Simple Storage Service 
f) hdfs file 
------------------------------------------

csv , json , jdbc,

when i go to spark shell 
spark libaries but it does not have the driver jar : 





1. file // .read.json
			.read.csv 
			.read.flatFiles 
2. i am going to run the  database : 
	url
	username
	password 
	driver 
3. jar file // maven url (mysql connector maven )
4. to run this on a spark-shell its important that you run the same with 
	--jars 
	spark-shell --jars c:\lib\mysql-connector-java-5.1.47.jar
5. if you are running this from your application 
	 <dependency>
                <groupId>mysql</groupId>
                <artifactId>mysql-connector-java</artifactId>
                <version>5.1.47</version>
            </dependency>
6. You will need a table 
	you open toad for mysql 
	you just connect to the db (username : root  / passwird : root123 )

val test_table =  """ (select * from test_table where name = 'nilesh') foo""";

7.  spark.read.format("jdbc").option("url","jdbc:mysql://localhost:3306/test").option("driver","com.mysql.jdbc.Driver").option("user","root").option("password","root")
.option("dbtable",test_table).load();



can i run a query and fetch the table first using 
-----------------------------------------------------------------------------------------




How to read xml file 
1. Step you will need to enable databricks support 
<dependency>
            <groupId>com.databricks</groupId>
            <artifactId>spark-xml_2.11</artifactId>
            <version>0.5.0</version>
        </dependency>
 2. understand the schema of your xml and define a rowTag 
 3. write the code 
  val sparkSession =
        SparkSession.builder.
          appName("xmldemo").
          master("local[*]").
          getOrCreate();
    val df = sparkSession.read.
              format("com.databricks.spark.xml").
              option("rowTag", "book").
              load("d:/inputData/books.xml");
    df.show();
 4. run the code with databricks 


==================================================================================
1. How can i parse more complex xml 
2. How can join data where keys not matching 
3. Can i use a csv which xml inside inside it 
----------------------------------------------------------------------------------


a) Can i read xml 
b)  can i read flat files 
c)  can i read xml 
d)  can i read json 
e)// how to write your own schemas for parse 
-------------------------------------------------------------------------------------

How to actually use the  Streaming data : 
for Streaming data ---> 
s3 ---> 

twitter ---> live twitter feed --> sentiment analysis on topic --> 
s3 ----> (Cloud based storage ---> putting files continously to the s3 )
	and as soon as i get the file i will be triggering the data reads 


there should be a host:port --> (Socket Read , Socket Writer )


If i want streaming data ---> Live Data ---> Live Feeds ---> Atom Feeds ->
Live constant feeds 

 <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.4.3</version>
  </dependency>

Simple Storage Service ---> cloud file store --> moinitor 
one add file --> wish trigger a immediate reading 

1. you should setup your pom.xml with the following 
	a) spark-core 
	    <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.4.3</version>
        </dependency>

	b) spark-sql
	<dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.4.3</version>
        </dependency>

	c) the spring-streaming
	    <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.4.3</version>
        </dependency>
 
    d)  Databricks (XML purpose )
		<dependency>
            <groupId>com.databricks</groupId>
            <artifactId>spark-xml_2.11</artifactId>
            <version>0.5.0</version>
        </dependency>

    e)  MySQL library to ensure we can read data from (jdbc )
     <dependency>
                <groupId>mysql</groupId>
                <artifactId>mysql-connector-java</artifactId>
                <version>5.1.47</version>
            </dependency>

    f) How to read data from streaming context from 
    	
    	adding 2 libraries 

	<dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-aws</artifactId>
            <version>2.7.1</version>
        </dependency>
        
        <!--- new version of Jackson which does not come with the hadoop library --> 

        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-annotations</artifactId>
            <version>2.9.9</version>
        </dependency>





i can read data on socket 
i can read data on file service (S3)
i can read data on kafka server message are being published 
i can read data from twitter api ... streaming 

Step1 : 
<dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>2.4.3</version>
        </dependency>

Step 2 : 
	We decide where read to the data ? 
	how bring stream on my local machine : - 


































Starting
----------
1. We have taken scala (Activities)
scala is function programming -->
2. scala repl shell
3. Scala maven / gradle/sbt setups
4. pom.xml
5. spark hdfs cluster and worker get setup
6. how we can write spark scala programs
7. spark shell
8. the spark sql
9. the spark streaming
10. spark dataframes
11. Spark rdd
12. spark the sql connect
13. spark application developement
14. spark core library
15. spark sql libary
16. spark application with streaming how stream data can be pushed
17. how to compile spark application with intellij
18. Example Stream data over s3 : You could put on s3
19. how the spark context works
20. sparkconfig
21. sparksession works
22. how the spark application makes use of the  DaG
21. how we can make use array and collections
22. Threads
21. Even the command line for spark
22. Mlib , the WebSevice , Cloud features
23. Joins
24. how the certification we can do and what we should read
25. we would see how we can  write application end to end
--------------------------------------------------------------------------------------
Code is in the git
nilesh.devdas@vinsys.com
--------------------------------------------------------------------------------------

1. Joins


Streaming context -->
they get the anaylisys done --->
data to web socket some kind listener =>
highcharts requires data from service ?

we should know how to write a rest service --> webservice

Scala + spark + webservice ---> Data is visible on xml / json over http
Write a service



How to write webservice with scala: so that my data is serviced over the webservices
apify (Creates api for my application);

1. Akka (is a Actor Libary for Creating , Spawing threads in the world of scala)
2. some api for webservice



        <!-- -->
        <dependency>
            <groupId>io.spray</groupId>
            <artifactId>spray-json_2.11</artifactId>
            <version>1.3.5</version>
        </dependency>

        <dependency>
            <groupId>io.spray</groupId>
            <artifactId>spray-can_2.11</artifactId>
            <version>1.3.4</version>
        </dependency>

        <dependency>
            <groupId>io.spray</groupId>
            <artifactId>spray-routing_2.11</artifactId>
            <version>1.3.4</version>
        </dependency>

        <dependency>
            <groupId>io.spray</groupId>
            <artifactId>spray-testkit_2.11</artifactId>
            <version>1.3.4</version>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-actor_2.11</artifactId>
            <version>2.5.23</version>
        </dependency>

        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-testkit_2.11</artifactId>
            <version>2.5.23</version>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.scalaz</groupId>
            <artifactId>scalaz-core_2.11</artifactId>
            <version>7.2.27</version>
        </dependency>



1. Add the above dependencies in the dependencies section of your pom.xml
2. Add rhe service.scala
3. ServiceActor.scala


Spark Core : RDD
Spark SQL  : json ,xml , csv, jdbc
Streaming  : socket, s3, hdfs , streaming
MLLib 	   : classification using RandomForest, Kmeans, Naive Bayes , Decision Trees
		Clustering ,  fpm , linalgebra , recommendations, regression

=====================================================================================
Spark can be used on the cloud
------------------------------------------------------------------------------------
Spark Cluster in local (Single Node)
----------------------------------------------------------------------------------
Spark Cluster in linux
----------------------------------------------------------------------------------
Spark Cluster With Worker and HDFS
----------------------------------------------------------------------------------
Parsed CSV , XML , JSON , JDBC , TEXT ,
=================================================================================
DataFrames , RDD
================================================================================
accumulators and broadcasts
---------------------------------------------------------------------------------
the spark ui , With Storage (Persist and cache )
---------------------------------------------------------------------------------
Maven and how to setup maven pom.xml
---------------------------------------------------------------------------------
spark-shell  spark-shell
----------------------------------------------------------------------------------
using spark with services (WebService Akkka and Spray )
==================================================================================

a) highcharts , html ,css, javascript for the front end

b) websockets , rest api for data retrival

c) service --> scala

d) spark streaming data

e) data source (socket, s3, hdfs )
Event triggers

====================================================================================




































































































































































	









































































































